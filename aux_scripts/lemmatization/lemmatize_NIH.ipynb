{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "319e2032",
   "metadata": {},
   "source": [
    "# Lemmatize NIH project abstracts using Spark NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58ccc6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "import sparknlp\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007ee1f5",
   "metadata": {},
   "source": [
    "## 1. Read projects and abstracts. Concatenate text fields\n",
    "\n",
    "**Important:** We will use Pandas to parse the files, because pySpark does not seem to process NIH CSV files correctly when there are additional '\"' in some of the files. Pandas seems to do it OK.\n",
    "\n",
    "We will also do some basic cleanup of the text fields. This is directly done in Pandas, it may be better to do it in Spark\n",
    "\n",
    "Result is saved in a CSV file in the local folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f380bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "NIHprojects = pd.read_csv('file:///export/data_ml4ds/IntelComp/Datasets/nih/csv/20220119/projects.csv', low_memory=False)\n",
    "NIHabstracts = pd.read_csv('file:///export/data_ml4ds/IntelComp/Datasets/nih/csv/20220119/abstracts.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a7646c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NIHprojects = NIHprojects[[\"APPLICATION_ID\", \"PROJECT_TITLE\", \"PROJECT_TERMS\", \"PHR\"]]\n",
    "NIHprojects = NIHprojects.fillna(\"\")\n",
    "NIHprojects[\"PROJECT_TERMS\"] = NIHprojects[\"PROJECT_TERMS\"].apply(lambda x: x.replace(\";\", \"; \"))\n",
    "NIHprojects[\"PHR\"] = (NIHprojects[\"PHR\"]\n",
    "                          .apply(str)\n",
    "                          .replace(\"PROJECT NARRATIVE \", \"\")\n",
    "                          .replace('\"', '')\n",
    "                          .apply(lambda x: \" \".join(x.split()))\n",
    "                     )\n",
    "NIHabstracts = NIHabstracts[[\"APPLICATION_ID\", \"ABSTRACT_TEXT\"]]\n",
    "NIHabstracts[\"ABSTRACT_TEXT\"] = (NIHabstracts[\"ABSTRACT_TEXT\"]\n",
    "                                    .apply(str)\n",
    "                                    .replace(\"PROJECT SUMMARY/ABSTRACT\", \"\")\n",
    "                                    .replace(\"PROJECT SUMMARY\", \"\")\n",
    "                                    .replace('\"', '')\n",
    "                                    .apply(lambda x: \" \".join(x.split()))\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b9ef828",
   "metadata": {},
   "outputs": [],
   "source": [
    "NIHprojects = NIHprojects.merge(NIHabstracts, how=\"inner\", on=\"APPLICATION_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1efc6a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NIHprojects[\"rawtext\"] = NIHprojects[\"PROJECT_TITLE\"] + \". \" + \\\n",
    "                         NIHprojects[\"ABSTRACT_TEXT\"] + \". \" + \\\n",
    "                         NIHprojects[\"PHR\"] + \". \" + \\\n",
    "                         NIHprojects[\"PROJECT_TERMS\"]\n",
    "NIHprojects = NIHprojects[[\"APPLICATION_ID\", \"rawtext\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ae8c46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe will be saved to temporary file\n",
    "NIHprojects.to_csv(\"./NIHrawtext.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d654e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/export/usuarios_ml4ds/jarenas/github/IntelComp/ITMT/topicmodeler/aux/lemmatization'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ba78d8",
   "metadata": {},
   "source": [
    "## 2. Filter abstracts that are not in English Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d4c95a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:===============================================>   (1857 + 143) / 2000]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of projects before language filtering: 2278732\n",
      "CPU times: user 44.2 ms, sys: 12.5 ms, total: 56.7 ms\n",
      "Wall time: 42 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NIHraw = spark.read.csv(\"file:///export/usuarios_ml4ds/jarenas/github/IntelComp/ITMT/topicmodeler/aux/lemmatization/NIHrawtext.csv\", header=True)\n",
    "NIHraw = NIHraw.repartition(2000)\n",
    "print(\"Number of projects before language filtering:\", NIHraw.count())\n",
    "#NIHraw.show(n=10, truncate=120, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba2ecc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ld_wiki_tatoeba_cnn_21 download started this may take some time.\n",
      "Approximate size to download 7.1 MB\n",
      "[ | ]ld_wiki_tatoeba_cnn_21 download started this may take some time.\n",
      "Approximate size to download 7.1 MB\n",
      "Download done! Loading the resource.\n",
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:=================================>                        (7 + 0) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[ — ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:===========================================>              (9 + 0) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[ \\ ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 6:====================================================>    (11 + 1) / 12]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[ | ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-12 12:51:00.039582: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:==================================================> (1956 + 44) / 2000]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of projects in English: 2231208\n",
      "CPU times: user 176 ms, sys: 31.8 ms, total: 208 ms\n",
      "Wall time: 3min 43s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Pipeline for language detection\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"rawtext\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "languageDetector = LanguageDetectorDL.pretrained() \\\n",
    "    .setInputCols(\"document\") \\\n",
    "    .setOutputCol(\"language\")\n",
    "\n",
    "pipeline = Pipeline() \\\n",
    "    .setStages([\n",
    "      documentAssembler,\n",
    "      languageDetector\n",
    "    ])\n",
    "\n",
    "#Apply language detection pipeline\n",
    "NIHraw = pipeline.fit(NIHraw).transform(NIHraw)\n",
    "NIHraw = (\n",
    "    NIHraw.filter(F.col(\"language.result\")[0]==\"en\")\n",
    "    .drop(\"language\")\n",
    ")\n",
    "\n",
    "print('Number of projects in English:', NIHraw.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4d59da",
   "metadata": {},
   "source": [
    "## 3. Define and Run Lemmatization Pipeline\n",
    "\n",
    "   - We work on documents created in Subsection 2\n",
    "   - Sentence Detection and Tokenizer applied to detect tokens\n",
    "   - Lemmatization is carried out\n",
    "   - Stopwords are applied\n",
    "   - Punctuation symbols are removed\n",
    "   - Result is converted back from Spark NLP annotations to string format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eecf0f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[ | ]lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "Download done! Loading the resource.\n",
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 11:==============================================>         (10 + 0) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[ — ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------------------------------------------------------------------------------------------------------\n",
      " APPLICATION_ID | 8685183                                                                                                                  \n",
      " rawtext        | Targeting Endoplasmic Reticulum Stress Response for Cancer Therapy. DESCRIPTION (provided by applicant): The overall ... \n",
      " lemmas         | targeting endoplasmic reticulum stress response cancer therapy description provide applicant overall goal proposal te... \n",
      "-RECORD 1----------------------------------------------------------------------------------------------------------------------------------\n",
      " APPLICATION_ID | 8839547                                                                                                                  \n",
      " rawtext        | Bifunctional antibodies with targeted CNS delivery against West Nile Virus. PROJECT SUMMARY/ABSTRACT West Nile Virus ... \n",
      " lemmas         | bifunctional antibody target cns delivery west nile virus project summaryabstract west nile virus wnv cause infection... \n",
      "-RECORD 2----------------------------------------------------------------------------------------------------------------------------------\n",
      " APPLICATION_ID | 8725080                                                                                                                  \n",
      " rawtext        | Targeted therapies to correct genomic instability in Brca1-deficient cells.. Project Abstract: Targeted therapies to ... \n",
      " lemmas         | targeted therapy correct genomic instability brca1deficient cell project abstract targeted therapy correct genomic in... \n",
      "-RECORD 3----------------------------------------------------------------------------------------------------------------------------------\n",
      " APPLICATION_ID | 9322533                                                                                                                  \n",
      " rawtext        | Interrupting the Vicious Cycle of Obesity and Metabolic Syndrome. ? DESCRIPTION (provided by applicant): The prevalen... \n",
      " lemmas         | interrupting vicious cycle obesity metabolic syndrome description provide applicant prevalence maternal overweight ob... \n",
      "-RECORD 4----------------------------------------------------------------------------------------------------------------------------------\n",
      " APPLICATION_ID | 9282512                                                                                                                  \n",
      " rawtext        | Statistical ICA Methods for Analysis and Integration of Multi-dimensional Data. DESCRIPTION (provided by applicant): ... \n",
      " lemmas         | statistical ica methods analysis integration multidimensional data description provide applicant study mental disorde... \n",
      "-RECORD 5----------------------------------------------------------------------------------------------------------------------------------\n",
      " APPLICATION_ID | 9238066                                                                                                                  \n",
      " rawtext        | Neuroimaging of Alcohol-Induced Neuroadaptation: Translation From Animals to Humans. PROJECT SUMMARY/ABSTRACT This co... \n",
      " lemmas         | neuroimaging alcoholinduced neuroadaptation translation animals humans project summaryabstract compete renewal applic... \n",
      "-RECORD 6----------------------------------------------------------------------------------------------------------------------------------\n",
      " APPLICATION_ID | 9528017                                                                                                                  \n",
      " rawtext        | ITM 2.0: Advancing Translational Science in Metropolitan Chicago. nan. . Abbreviations;  Academic Medical Centers;  A... \n",
      " lemmas         | itm 20 advancing translational science metropolitan chicago nan abbreviations academic medical centers accreditation ... \n",
      "-RECORD 7----------------------------------------------------------------------------------------------------------------------------------\n",
      " APPLICATION_ID | 9710605                                                                                                                  \n",
      " rawtext        | Tissue-specific tumor suppressor effects of p53. ? DESCRIPTION (provided by applicant): The tumor suppressor p53 has ... \n",
      " lemmas         | tissuespecific tumor suppressor effect p53 description provide applicant tumor suppressor p53 implicate play key role... \n",
      "-RECORD 8----------------------------------------------------------------------------------------------------------------------------------\n",
      " APPLICATION_ID | 9752980                                                                                                                  \n",
      " rawtext        | Investigation of Cognitive Impairment in Parkinson Disease. Parkinson Disease (PD) causes progressive motor and non-m... \n",
      " lemmas         | investigation cognitive impairment parkinson disease parkinson disease pd cause progressive motor nonmotor deficit 75... \n",
      "-RECORD 9----------------------------------------------------------------------------------------------------------------------------------\n",
      " APPLICATION_ID | 9686745                                                                                                                  \n",
      " rawtext        | Algorithm and neural basis of a fundamental visual motion computation. ? DESCRIPTION (provided by applicant): Visual ... \n",
      " lemmas         | algorithm neural basis fundamental visual motion computation description provide applicant visual motion perception c... \n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 224 ms, sys: 48.7 ms, total: 273 ms\n",
      "Wall time: 3min 40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "#Next, we carry out the lemmatization pipeline\n",
    "\n",
    "sentenceDetector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"lemma\")\n",
    "\n",
    "stopWords = StopWordsCleaner() \\\n",
    "    .setInputCols([\"lemma\"]) \\\n",
    "    .setOutputCol(\"cleanlemma\")\n",
    "\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"cleanlemma\"]) \\\n",
    "    .setOutputCol(\"normalizedlemma\") \\\n",
    "    .setLowercase(True) \\\n",
    "    .setCleanupPatterns([\"\"\"[^\\w\\d\\s]\"\"\"])\n",
    "\n",
    "finisher = Finisher() \\\n",
    "     .setInputCols(['normalizedlemma'])\n",
    "\n",
    "pipeline = Pipeline() \\\n",
    "    .setStages([\n",
    "      sentenceDetector,\n",
    "      tokenizer,\n",
    "      lemmatizer,\n",
    "      stopWords,\n",
    "      normalizer,\n",
    "      finisher\n",
    "])\n",
    "\n",
    "#We apply pipeline and recover lemmas as string\n",
    "NIHraw = pipeline.fit(NIHraw).transform(NIHraw)\n",
    "\n",
    "udf_back2str = F.udf(lambda x:' '.join(list(x)), StringType() )\n",
    "NIHraw = (\n",
    "    NIHraw.withColumn(\"lemmas\",udf_back2str(F.col(\"finished_normalizedlemma\")))\n",
    "    .drop(\"finished_normalizedlemma\")\n",
    ")\n",
    "\n",
    "#Show results of validation for n papers\n",
    "NIHraw.show(n=10, truncate=120, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f63e174",
   "metadata": {},
   "source": [
    "## 4. Save a table with `APPLICATION_ID` and `lemmas` to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4c5b911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 458 ms, sys: 102 ms, total: 560 ms\n",
      "Wall time: 6min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Save calculated lemmas to HDFS\n",
    "dir_parquet = Path(\"/export/ml4ds/IntelComp/Datalake/NIH/20220119\")\n",
    "\n",
    "NIHraw.coalesce(1000).write.parquet(\n",
    "    dir_parquet.joinpath(f\"projects_NLP.parquet\").as_posix(),\n",
    "    mode=\"overwrite\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb1450a",
   "metadata": {},
   "source": [
    "## 5. Delete temporary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78fc54f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file deleted\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "file = \"./NIHrawtext.csv\"\n",
    "\n",
    "if(os.path.exists(file) and os.path.isfile(file)):\n",
    "    os.remove(file)\n",
    "    print(\"file deleted\")\n",
    "else:\n",
    "    print(\"file not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c19228",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
